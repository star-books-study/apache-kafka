# Chapter 4. 카프카 상세 개념 설명

## 4.1. 토픽과 파티션
### 4.1.1. 적정 파티션 개수
- 토픽 생성 시 파티션 개수 고려사항
  - 데이터 처리량
  - 메시지 키 사용여부
  - 브로커, 컨슈머 영향도
- 파티션은 카프카의 `병렬처리`의 핵심이다. 파티션의 개수가 많아지면 많아질수록 1:1 매핑되는 컨슈머 개수가 늘어나기 때문이다.
- 데이터 처리 속도 올리는 방법
  - 컨슈머의 **처리량**을 늘리는 것
    - 서버의 사양을 올리는 스케일 업 or GC 튜닝 활용 가능
  - **컨슈머를 추가해서 병렬 처리량**을 늘리는 것
- 프로듀서가 보내는 데이터양, 컨슈머의 데이터 처리량을 계산해서 파티션 개수를 정하면 된다
  - 프로듀서 전송 데이터양 < 컨슈머 데이터 처리량 * 파티션 개수
  - 전체 컨슈머 데이터 처리량 < 프로듀서 전송 데이터일 경우, **컨슈머 랙이 생기고 데이터 처리 지연이 발생하게 된다**
- 컨슈머 데이터 처리량을 구하는 방법 : 상용에서 운영 중인 카프카에서 더미 데이터로 테스트를 해보는 것
- 컨슈머 데이터 처리량을 구하고 나면, 프로듀서가 보내는 데이터양을 하루,시간,분 단위로 쪼개서 예측한다
  - 만약 데이터 지연이 절대 발생해선 안된다면, 프로듀서가 보내는 데이터의 최대치를 데이터 생성량으로 잡고 계산하면 된다
- **메시지키를 사용함과 동시에 데이터 처리 순서를 지켜야 하는 경우**에 대해 고려해야 한다
  - 기본 파티셔너 사용 시, 메시지 키를 사용하면 프로듀서가 토픽으로 데이터를 보낼 때, 메시지 키를 해시 변환하여 파티션에 매칭시킨다
  - 그러므로 파티션 개수가 달라지는 순간, 메시지 키를 사용하는 컨슈머는 특정 메시지 키의 순서를 더는 보장받지 못한다
  - **메시지 키를 사용하고 컨슈머에서 메시지 처리 순서가 보장되어야 한다면 최대한 파티션의 변화가 발생하지 않는 방향으로 운영해야 한다**
    > - Kafka는 기본적으로 "하나의 파티션 내부"에서만 메시지 순서를 보장합니다.
    > - 메시지를 보낼 때 key를 지정하면 같은 key의 메시지는 항상 같은 파티션으로 가게 되어, 그 파티션에 대해 **단일 컨슈머 스레드**가 순차적으로 메시지를 처리하면 순서 보장이 가능합니다.
    > 
    > | 조건                  | 순서 보장 여부 | 설명                                           |
    > |-------------------|----------------|------------------------------------------------|
    > | 하나의 파티션 → 하나의 컨슈머 → 단일 스레드 처리 | ✅ 보장됨        | Kafka의 파티션 순서 + 순차 처리로 순서 유지     |
    > | 하나의 파티션 → 하나의 컨슈머 → 멀티스레드 처리 | ❌ 깨질 수 있음   | 메시지를 병렬 처리하는 순간, 순서가 뒤섞일 수 있음 |
  - 파티션 개수가 변해야 하는 경우에는 **기존에 사용하던 메시지 키의 매칭을 그대로 가져가기 위해 커스텀 파티셔너를 개발하고 적용**해야 한다
  - 메시지 키별로 처리 순서를 보장하려면, 파티션 개수를 프로듀서가 전송하는 데이터양보다 더 넉넉하게 잡고 생성하는 것이 좋다
- 카프카에서 파티션은 각 브로커의 `파일시스템`을 이용하므로, 파티션이 늘어나는 만큼 브로커에서 접근하는 파일 개수가 많아진다. 
  - 하지만 운영체제에서는 프로세스당 열 수 있는 파일 최대 개수를 제한하고 있다.
  - 만약, 브로커가 관리하는 파티션 개수가 너무 많을 경우, 파티션 개수를 분산하기 위해 카프카 브로커 개수를 늘리는 방안도 같이 고려해야 한다

### 4.1.2. 토픽 정리 정책 (cleanup.policy)
- 데이터를 더이상 사용하지 않을 경우 cleanup.policy 옵션으로 삭제할 수 있다
  - delete (완전 삭제) - 데이터 완전 삭제
  - compare (압축) - 동일 메시지 키의 가장 오래된 데이터를 삭제

#### 토픽 삭제 정책 (delete policy)
- 토픽의 데이터 삭제 시 `세그먼트` 단위로 삭제를 진행한다
  - 세그먼트 : 토픽의 데이터를 저장하는 **명시적인 파일 시스템 단위**

![alt text](image-1.png)
- 세그먼트는 파티션마다 별개로 생성되며 세그먼트의 파일 이름은 오프셋 중 가장 작은 값이 된다
- 세그먼트는 여러 조각으로 나뉘는데 segment.bytes 옵션으로 1개의 세그먼트 크기를 설정할 수 있다
- segment.bytes 크기보다 커질 경우에는 **기존에 적재하던 세그먼트 파일을 닫고 새로운 세그먼트를 열어서 데이터를 저장한다.**
- 액티브 세그먼트 : 데이터를 저장하기 위해 사용 중인 세그먼트
- 삭제 정책이 실행되는 시점 : 시간 or 용량
  - `retention.ms` : 토픽의 데이터를 유지하는 기간을 밀리초(millisecond)로 설정할 수 있다
    - 카프카는 일정 주기마다 세그먼트 파일의 마지막 수정 시간과 retention.ms를 비교하는데, 세그먼트 파일의 마지막 수정 시간이 retention.ms를 넘어가면 세그먼트는 삭제된다
  - `retention.bytes` : 토픽의 최대 데이터 크기 제어한다
    - retention.bytes 를 넘어간 세그먼트 파일들은 삭제된다
  
#### 토픽 압축 정책 (compact policy)
- 압축 : 메시지 키별로 해당 **메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책**
  - 압축 정책은 액티브 세그먼트를 제외한 나머지 세그먼트들에 한함!
  - ![alt text](image-2.png)
- `테일 영역 (클린 로그)` : 압축이 완료됐기에 중복된 메시지 키가 없다
- `헤드 영역 (더티 로그)` : 압축 되기 전 레코드가 존재하므로 중복된 메시지를 가진 레코드들이 있다
- 토픽의 압축은 `min.cleanable.dirty.ratio` 값에 따라 수행된다.
  - ex. 0.5 로 설정할 경우 더티 비율 (더티 레코드 개수 / (클린 + 더티 레코드 개수)) 이 0.5 가 넘어가면 압축이 수행됨
  - 0.9 와 같이 크게 비율 설정 시, **한번 압축 시 많은 데이터가 줄어드므로 압축 효과가 좋다.** <br> 하지만 0.9 비율이 될 때까지 용량을 차지하므로 용량 효율이 좋지 않다
  - 0.1 과 같이 작게 비율 설정 시, 압축이 자주 일어나므로 **메시지 키의 최신 데이터만 유지할 수 있지만 자주 압축이 발생하는 만큼 브로커에 부담을 줄 수 있다**

### 4.1.3. ISR (In-Sync-Replicas)
- **ISR** : 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태
  - 동기화 완료 == 리더 파티션의 모든 데이터가 팔로워 파티션에 복제된 상태
  - ![alt text](image-3.png)
- ISR 용어가 등장한 배경은, 팔로워 파티션이 리더 파티션으로부터 데이터 복제하는데에는 시간이 걸리기 때문이다
  - 프로듀서가 특정 파티션에 데이터를 저장하는 작업은 리더 파티션을 통해 처리한다
- **리더 파티션에 데이터가 적재된 이후 팔로워 파티션이 복제하는 시간차 때문에 리더 파티션과 팔로워 파티션 간에 오프셋 차이가 발생**
  - 이 차이를 모니터링하기 위해, 리더 파티션은 `replica.lag.time.max.ms` 값만큼의 주기를 갖고 팔로워 파티션이 데이터를 복제하는지 확인
  - 팔로워 파티션이 위 옵션값보다 **더 긴 시간동안 데이터를 가져가지 않을 경우, 팔로워 파티션에 문제가 생긴 것으로 판단하고 ISR 그룹에서 제외**
- ISR로 묶인 리더 파티션과 팔로워 파티션은 파티션에 존재하는 데이터가 모두 동일하기 때문에 팔로워 파티션은 리더 파티션으로 새로 선출될 자격을 가진다
  - 일부 데이터 유실이 발생하더라도, 서비스 중단 없이 지속적으로 토픽을 사용하고 싶다면 ISR 이 아닌 팔로워 파티션을 리더로 선출하도록 설정도 가능하다
    - `unclean.leader.election.enable` : ISR이 아닌 팔로워 파티션을 리더 파티션으로 선출 가능한지 여부를 선택을 위한 옵션
      - true : 리더 파티션이 존재하는 브로커에서 장애가 발생하고 동기화되지 않은 팔로워 파티션이 리더로 선출되면 **리더 파티션으로부터 동기화가 되지 않은 일부 데이터는 유실될 수 있다. 일부 데이터가 유실되는 대신 토픽을 사용하는 서비스의 중단은 발생하지 않는다**
      - false : 리더 파티션이 존재하는 브로커가 다시 시작되기까지 기다린다. **리더 파티션이 존재하는 브로커가 다시 시작될 때까지 기다리는 것은 토픽을 사용하는 서비스가 중단됨**을 뜻한다. 장애가 발생한 브로커가 다시 실행될 때까지 해당 토픽은 사용할 수 없다
- 데이터가 유실되더라도 토픽과 연동 중인 서비스의 무중단 운영이 더 중요하다면 true로 설정하면 되고, 반면, 데이터가 유실되면 안 되는 경우에는 false로 설정 해야 한다.

## 4.2. 카프카 프로듀서
### 4.2.1. acks 옵션
- 이 옵션을 통해 **프로듀서가 전송한 데이터가 카프카 클러스터에 얼마나 신뢰성 높게 저장할지 지정**할 수 있다
  - acks옵션에 따라 성능이 달라질 수 있으므로 acks 옵션에 따른 카프카의 동작 방식을 상세히 알고 설정해야 한다

#### acks=0
- acks를 0으로 설정하는 것 : 프로듀서가 리더 파티션으로 데이터를 전송했을 때 **리더 파티션으로 데이터가 저장되었는지 확인하지 않는다**
- 리더 파티션은 데이터가 저장된 이후에 데이터가 몇 번째 오프셋에 저장되었는지 리턴하는데, acks가 0으로 설정되어 있다면 **리더 파티션에 데이터 저장된 지 여부에 대한 응답값을 받지 않는다.**
- acks가 0일 때 프로듀서는 전송을 하자마자 데이터가 저장되었음을 가정하고 다음 데이터를 전송하기 때문에 데이터 전송이 실패한 경우를 알 수 없다. 
  - 따라서 retries가 2 이상으로 설정되어 있더라도 재시도를 하지 않기 때문에 retries 옵션값은 무의미하다.
- 데이터의 전송 속도는 acks를 1 또는 all로 했을 경우보다 훨씬 빠르다. <br>**데이터가 일부 유실이 발생하더라도 전송 속도가 중요한 경우에는 이 옵션값을 사용하면 좋다.**

#### acks=1
- acks를 1로 설정할 경우 **프로듀서는 보낸 데이터가 리더 파티션에만 정상적으로 적재되었는지 확인한다.**
  - 만약 리더 파티션에 정상적으로 적재되지 않았다면 리더 파티션에 적재될 때까지 재시도할 수 있다
- 팔로워 파티션에는 아직 데이터가 동기화되지 않을 수 있는데, **팔로워 파티션이 데이터를 복제하기 직전에 리더 파티션이 있는 브로커에 장애가 발생하면 동기화되지 못한 일부 데이터가 유실될 가능성이 있다.**

#### acks=all 또는 acks=-1
- acks를 all 또는 -1로 설정할 경우 **프로듀서는 보낸 데이터가 리더 파티션과 팔로워 파티션에 모두 정상적으로 적재되었는지 확인한다**
  - 팔로우 파티션에 데이터가 정상 적재되었는지 기다리기 때문에 **일부 브로커에 장애가 발생하더라도 프로듀서는 안전하게 데이터를 전송하고 저장할 수 있음을 보장할 수 있다**
- all 옵션값은 모든 리더 파티션과 팔로워 파티션의 적재를 뜻하는 것은 아니고, `ISR에 포함된 파티션`들을 뜻한다
- `min.insync.replicas` = 1이라면 ISR 중 최소 1개 이상의 파티션에 데이터가 적재되었음을 확인하는 것이다. <br> -> **이 경우 acks를 1로 했을 때와 동일한 동작을 하는데, 왜냐하면 ISR 중 가장 처음 적재가 완료되는 파티션은 리더 파티션이기 때문이다**
- min.insync.replicas의 옵션값을 2로 설정했을 때부터 acks를 all로 설정하는 의미가 있다.
  - 실제 카프카 클러스터를 운영하면서 브로커가 동시에 2개가 중단되는 일은 극히 드물기 때문에 리더 파티션과 팔로워 파티션 중 1개에 데이터가 적재 완료되었다면 데이터는 유실되지 않는다고 볼 수 있다
- 운영하는 카프카 브로커 개수 < min.insync.replicas의 옵션값 인 경우, 프로듀서가 더는 데이터를 전송할 수 없기 때문이다
  - 최소한으로 복제되어야 하는 파티션 개수가 3인데 팔로워 파티션이 위치할 브로커의 개수가 부족하면 NotEnoughReplicasException 또는 NotEnoughReplicasAfterAppen dException이 발생
  - **토픽별 min.insync.replicas 옵션값은 브로커 개수 미만으로 설정해서 운영해야 한다**
  - 상용환경에서는 일반적으로 브로커를 3대 이상으로 묶어 클러스터를 운영하는데, 이 점을 고려하여 **프로듀서가 데이터를 가장 안정적으로 보내려면 토픽의 복제 개수는 3, min.insync.replicas를 2로 설정하고 프로듀서는 acks를 all로 설정하는 것을 추천한다.**

### 4.2.2. 멱등성 (idempotence) 프로듀서
- 멱등성 프로듀서 : 동일 데이터를 **여러 번 전송해도 카프카 클러스터에 단 한번만 저장됨**
  - 기본 프로듀서 동작 방식 : 적어도 한번 전달 (at least once delivery)
    - 프로듀서가 클러스터에 데이터를 전송하여 저장할 때, 적어도 한 번 이상 데이터 데이터 적재 가능하며 데이터가 유실되지 않음
- `enable.indempotence` : 정확히 한번 전달을 위해서는 true 로 설정하여 멱등성 프로듀서로 동작하도록 만들어야 함
  - 멱등성 프로듀서는 기본 프로듀서와 달리 데이터를 브로커로 전달 시, **프로듀서 PID(Producer unique ID)와 시퀀스 넘버(sequence number)를 함께 전달**
  - 브로커는 프로듀서의 PID와 시퀀스 넘버를 확인 <br> -> 동일한 메시지의 적재 요청이 오더라도 단 한 번만 데이터를 적재 <br> -> 프로듀서의 데이터는 정확히 한번 브로커에 적재됨
- 멱등성 프로듀서는 `동일한 세션에서`만 정확히 한번 전달을 보장함 !
  - 동일한 세션 == PID 의 생명 주기 (즉, 프로듀서 애플리케이션에 이슈가 있어 종료 후 재시작하면 PID 가 달라짐)
  - 동일 데이터를 보내도 PID 가 달라지면, 브로커 입장에서는 다른 프로듀서로 판단함

![alt text](image-4.png)
- 브로커에서 멱등성 프로듀서가 전송한 데이터의 PID와 시퀀스 넘버를 확인하는 과정에서, 시퀀스 넘버가 일정하지 않은 경우에는 OutOfOrderSequenceException이 발생할 수 있다
  > 시퀀스 넘버가 일정하지 않게 되는 예시 상황
  > - 1. 프로듀서가 갑자기 종료되고 다시 시작 → 새로운 PID 할당 → 이전 세션의 시퀀스 번호보다 낮은 번호로 메시지를 보냄 → 브로커 입장에서는 "역전"이라고 판단
  >   - [이전 프로듀서 세션] <br>PID=123, SeqNum=0, 1, 2, 3
  >   - [새로운 프로듀서 세션] <br>PID=456, SeqNum=0 ← 브로커는 PID가 바뀌었으니 새 시퀀스로 인식 → OK
  > - 2. 하나의 KafkaProducer 인스턴스를 여러 스레드에서 공유하거나, 복제된 인스턴스가 동일한 PID를 가지면, 서로 다른 쓰레드나 인스턴스가 동일 파티션에 중복된/꼬인 시퀀스를 전송할 수 있음
  - OutOfOrderSequenceException이 발생했을 경우에는 시퀀스 넘버의 역전현상이 발생할수 있기 때문에 **순서가 중요한 데이터를 전송하는 프로듀서는 해당 Exception이 발생했을 경우 대응하는 방안을 고려해야 한다.**

### 4.2.3. 트랜잭션 (transaction) 프로듀서
- 트랜잭션 프로듀서는 **다수의 파티션에 데이터를 저장할 경우 모든 데이터에 대해 동일한 원자성(atomic)을 만족시키기 위해 사용**된다
  - 원자성 만족시킨다 == 다수 데이터를 동일 트랜잭션으로 묶어서 전체 데이터 처리하거나, 처리하지 않도록 하는 것 (all or nothing)
- enable.idempotence : true
- transactional.id 를 임의의 String값으로 정의
- 컨슈머의 isolation.level을 read_committed 로 설정 <br>
→ 프로듀서와 컨슈머는 트랜잭션으로 처리 완료된 데이터만 쓰고 읽게 된다

![alt text](image-5.png)
- 트랜잭션 프로듀서는 사용자가 보낸 데이터를 레코드로 파티션에 저장 + **트랜잭션의 시작과 끝을 표현하기 위해 트랜잭션 레코드를 1개 더 보낸다.** 
  - 트랜잭션 컨슈머는 파티션에 저장된 **트랜잭션 레코드를 보고 트랜잭션이 완료(commit)되었음을 확인**하고 데이터를 가져간다.
  - 트랜잭션 레코드는 실질적 데이터 X, 트랜잭션이 끝난 상태를 표시하는 정보만 가짐
  - 트랜잭션 컨슈머는 **커밋이 완료된 데이터가 파티션에 있을 경우에만 데이터를 가져간다.**
    - 만약 데이터만 존재하고 트랜잭션 레코드가 존재하지 않으면 아직 트랜잭션이 완료되지 않았다고 판단하고 데이터를 가져가지 않는다.
> ✅ 트랜잭션 프로듀서 실사례
> 1. DB → Kafka 싱크 연동 (CDC, ETL 등)
>    - DB 커밋됐을 때만 Kafka에도 메시지를 전송하고 싶다.
> 2. 여러 토픽에 전송할 때 원자성 보장
>    - **두 개 이상의 토픽에 동시에 메시지를 보내되, 일부만 성공해서는 안 된다.**

## 4.3. 카프카 컨슈머
### 4.3.1. 멀티 스레드 컨슈머
- 파티션 개수가 N개라면, 동일 컨슈머 그룹으로 묶인 컨슈머 스레드를 최대 N개 운영할 수 있다
  - N개의 스레드를 가진 1개 프로세스 운영 or 1개의 스레드를 가진 프로세스 N개 운영
  - ![](image-6.png)
  > ❓ **컨슈머 그룹으로 묶지 않고 컨슈머 스레드 여러 개로 운영하면 안되는가?**
  > - 컨슈머 그룹 없이 개별 컨슈머만 돌리면...
  >    - 각 컨슈머가 같은 데이터를 중복 소비하거나
  >    - Kafka가 파티션을 자동으로 할당해주지 않음 (카프카는 **컨슈머 그룹 단위로 파티션 분배**)
  >    - 처리 순서가 꼬일 수 있음
  >    - 수평 확장이나 장애 복구가 불가능
  > - 왜 컨슈머 그룹으로 운영하나?
  >    - **하나의 파티션은 하나의 컨슈머 그룹 안에서 오직 하나의 컨슈머만 읽을 수 있기 때문에, 메시지 중복 없이 병렬 처리하기 위함**
  > - 그룹이 다르면?
  >    - 같은 메시지를 각기 다르게 소비 가능 (멀티 파이프라인)
  > - 스레드만 여러 개 쓰면?	
  >    - 파티션 충돌, 중복 소비, 리밸런싱 불가 등 문제 발생
  > - 결국 "컨슈머 그룹 없이 병렬 처리" = Kafka 설계 철학에 맞지 않는 비권장 패턴
- 멀티 스레드로 컨슈머를 안전하게 운영하기 위해서는 고려할 부분이 많은데, 하나의 컨슈머 스레드에서 예외 상황 (OOM) 발생 시 프로세스 자체가 종료될 수도 있고, 이는 다른 컨슈머 스레드에까지 영향을 미칠 수 있다
  - 따라서 **각 컨슈머 스레드 간에 영향을 미치지 않도록 스레드 세이프 로직, 변수를 적용해야 한다**
- 컨슈머를 멀티 스레드로 활용하는 방식
  1. `멀티 워커 스레드` 전략 :  **컨슈머 스레드 1개 실행 / 데이터 처리를 담당하는 워커 스레드 여러 개** 실행
  2. `컨슈머 멀티 스레드` 전략 : **컨슈머 인스턴스에서 poll() 메서드 호출하는 스레드 여러개 띄워서** 사용 

#### 카프카 컨슈머 멀티 워커 스레드 전략
- 멀티 스레드를 생성하는 ExecutorService 자바 라이브러리를 사용하면 레코드를 병렬처리하는 스레드를 효율적으로 생성하고 관리할 수 있다
  - 작업 이후 스레드가 종료되어야 한다면 CachedThreadPool을 사용하여 스레드를 실행한다
    - newCachedThreadPool 은 필요한 만큼 스레드 풀을 늘려서 스레드를 실행하는 방식으로, 짧은 시간의 생명주기를 가진 스레드에서 유용하다

![alt text](image-7.png)
```java
public class ConsumerWorker implements Runnable { 
  private String recordValue;
  
  ConsumerWorker(String recordValue) { 
    this.recordValue = recordValue;
  }

  @Override public void run() { 
    logger.info("thread:{}\trecord:{}", Thread.currentThread().getName(), recordValue);
  } 
}
```
- 데이터를 처리하는 스레드를 개별로 생성하기 위해 **데이터를 처리하는 사용자 지정 스레드를 새로 생성**
  - Runnable 인터페이스로 구현한 클래스이므로 스레드로 실행되며, 생성되고 나면 run() 메서드가 실행되어 이 메소드에서 데이터가 처리됨
```java
KafkaConsumer<String, String> consumer = new KafkaConsumer<>(configs);
consumer.subscribe(Arrays.asList(TOPIC_NAME));
ExecutorService executorService = Executors.newCachedThreadPool();

while (true) {
  ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(10));
  for (ConsumerRecord<String, String> record : records) {
    ConsumerWorker worker = new ConsumerWorker(record.value());
    executorService.execute(worker);
  }
}
```
- 데이터를 처리하는 ConsumerWorker 클래스로, **poll() 메서드를 통해 리턴받은 레코드들을 처리하는 스레드를 레코드마다 개별 실행한다**
- 스레드를 사용하면 한번 poll()을 통해 받은 데이터를 `병렬처리` 함으로써 속도의 이점을 확실히 얻을 수 있다. 

주의사항
1. 스레드를 사용함으로써 **데이터 처리가 끝나지 않았음에도 불구하고 커밋을 하기 때문에 리밸런싱, 컨슈머 장애 시에 데이터 유실이 발생할 수 있다**
    - 위 코드는 각 레코드의 데이터 처리가 끝났음을 스레드로부터 리턴받지 않고 바로 그 다음 poll() 메서드를 호출한다
    - **오토 커밋일 경우 데이터 처리가 스레드에서 진행 중임에도 불구하고 다음 poll() 메서드 호출 시에 커밋을 할수 있기 때문에 발생하는 현상**이다. 
    > ➕ enable.auto.commit = true 일 경우에는 설정된 주기에 따라 KafkaConsumer가 백그라운드 스레드로 커밋을 함. <br> 
    > 1. 메시지를 poll()로 가져온 후,
    > 2. 각 레코드를 별도 스레드로 비동기 처리하고,
    > 3. KafkaConsumer는 그 다음 poll()을 호출
    > 4. 이 과정에서 자동 커밋이 동작하면, 아직 처리 중인 메시지도 커밋된 것으로 간주됨
    > - 스레드에서 예외가 발생하거나 처리 도중 종료되면 → 해당 메시지는 처리 실패하지만 커밋되어 다시 처리되지 않음
    > - 컨슈머가 다운되거나 리밸런싱되면 → 처리 중이던 메시지가 사라짐
    > 
    > ✅ 해결 방안
    > - 수동 커밋 사용 (enable.auto.commit=false)
    > - 스레드 작업 완료를 추적할 메커니즘 필요
    >   - ex. Future, CountDownLatch, ExecutorService.invokeAll() 등을 사용해 각 레코드의 처리가 완료된 후에만 커밋하도록 설계
    > - 처리 결과를 별도 저장소에 기록하는 패턴
    >   - DB나 로그 저장소에 저장 후, 메시지는 중복 처리 허용 기반으로 설계 (idempotent)


1. **레코드 처리의 역전현상**
   - for 문으로 스레드를 생성하므로 레코드별로 스레드의 생성은 순서대로 진행되지만, 스레드의 처리 시간은 다를 수 있다. 
   - 나중에 생성된 스레드의 레코드 처리 시간이 더 짧을 경우 이전 레코드가 다음 레코드보다 나중에 처리될 수 있다. 
   - **레코드 처리에 있어 중복이 발생하거나 데이터의 역전현상이 발생해도 되며 매우 빠른 처리 속도가 필요한 데이터 처리에 적합하다.**
     - ex. 서버 리소스(CPU, 메모리 등) 모니터링 파이프라인, IoT 서비스의 센서 데이터 수집 파이프라인

#### 카프카 컨슈머 멀티 스레드 전략
- **토픽의 파티션 개수만큼 컨슈머 스레드 개수를 늘려서 운영하는 것**이다. 
  - 컨슈머 스레드를 늘려서 운영하면 각 스레드에 각 파티션이 할당되며, 파티션의 레코드들을 병렬처리 할 수 있다.

![alt text](image-8.png)

```java
public class ConsumerWorker implements Runnable {
    private final static Logger logger = LoggerFactory.getLogger(ConsumerWorker.class);
    private Properties prop;
    private String topic;
    private String threadName;
    private KafkaConsumer<String, String> consumer; 
    // kafkaConsumer 는 스레드 세이프 하지 않은 클래스이므로, 스레드별로 별개의 인스턴스를 만들어서 운영해야 한다

    ConsumerWorker(Properties prop, String topic, int number) {
        this.prop = prop;
        this.topic = topic;
        this.threadName = "consumer-thread-" + number;
    }

    @Override
    public void run() {
        consumer = new KafkaConsumer<>(prop);
        consumer.subscribe(Arrays.asList(topic));
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofSeconds(1));
            for (ConsumerRecord<String, String> record : records) {
                logger.info("{}", record);
            }
            consumer.commitSync();
        }
    }
}
```

```java
public static void main(String[] args) {
  Properties configs = new Properties();
  ...(설정 중략)...

  ExecutorService executorService = Executors.newCachedThreadPool();
  for (int i = 0; i < CONSUMER_COUNT; i++) {
    ConsumerWorker worker = new ConsumerWorker(configs, TOPIC_NAME, i);
    executorService.execute(worker);
  }
}
```
- 위 코드를 통해 1개의 애플리케이션에 N 개의 컨슈머 스레드를 띄울 수 있다
- 스레드 하나 당 카프카 컨슈머 1개씩을 담당하여 후처리 및 커밋 로직이 동작한다

### 4.3.2. 컨슈머 랙
- 컨슈머 랙 (LAG) : 최신 오프셋 (log-end-offset), 컨슈머 오프셋 (current-offset) 간의 차이
- 컨슈머 랙은 **컨슈머가 정상 동작하는지 여부를 확인할 수 있는** 지표이다
- 컨슈머 랙은 컨슈머 그룹과 토픽, 파티션 별로 생성된다
  > - 컨슈머 랙은 '컨슈머 그룹 + 토픽 + 파티션' 단위로 존재한다는 말은, Kafka에서는 메시지 처리가 얼마나 밀렸는지를 각 컨슈머 그룹이 어떤 토픽의 어떤 파티션을 소비하고 있는지 기준으로 개별적으로 측정한다는 뜻
  > - 즉, **같은 토픽이더라도, 다른 컨슈머 그룹은 각자 메시지를 얼마나 읽었는지가 다르기 때문에 랙도 그룹마다, 파티션마다 따로 계산**됨
  >    - 컨슈머 그룹마다 동일한 파티션에 대해 컨슈머 랙이 다를 수 있음 
- 프로듀서가 보내는 데이터양 < 컨슈머의 데이터 처리량 일 경우, 컨슈머 랙은 줄어들고 최솟값은 0으로 지연이 없음을 뜻한다

#### 카프카 명령어를 사용하여 컨슈머 랙 조회
- kafka-consumer-groups.sh 명령어로 컨슈머 랙을 포함한 특정 컨슈머 그룹의 상태를 확인할 수 있다

#### 컨슈머 metrics() 메서드를 사용하여 컨슈머 랙 조회
- 컨슈머가 정상동작할 경우에만 확인 가능
  - 비정상적으로 종료된 컨슈머 애플리케이션의 경우, 컨슈머 랙 모니터링 불가
- 모든 컨슈머 애플리케이션에 컨슈머 랙 모니터링 코드 중복 작성 필요
  - 특정 컨슈머 그룹에 해당하는 애플리케이션이 수집하는 컨슈머 랙은 자기 자신 컨슈머 그룹에 대한 컨슈머 랙만 한정되기 때문
- 컨슈머 랙을 모니터링하는 코드 추가할 수 없는 서드파티 애플리케이션의 경우 컨슈머 랙 모니터링 불가

#### 외부 모니터링 툴을 사용하여 컨슈머 랙 조회
- 컨슈머 랙 모니터링하는 가장 최선의 방법은 외부 모니터링 툴을 사용하는 것

#### 4.3.2.1. 카프카 버로우
- 카프카 버로우 : 링크드인에서 공개한 오픈소스 컨슈머 랙 체크 툴
  - 버로우는 다수의 카프카 클러스터를 동시에 연결하여 컨슈머 랙을 확인한다
- 버로우는 **데이터 적재 기능이 없으므로 현시점 이후의 컨슈머 랙 지표만 조회 가능하다**
  - 따라서 별도 저장소 (인플럭스디비, 엘라스틱서치 등) 에 저장하고 대시보드 (그라파나, 키바나 등) 을 통해 조회, 알람 설정하는 것이 유지보수 시 편리하다
- 프로듀서가 데이터를 많이 보내면 `일시적`으로 임계치가 넘어가는 현상이 발생할 수 있으므로, 컨슈머 랙이 임계치에 도달할 때마다 알람을 받는 것은 무의미한 일이다 (=컨슈머 또는 파티션에 이슈가 있다고 단정지을 수는 없기 때문 !)

### 4.3.3. 컨슈머 배포 프로세스

#### 중단 배포
- 중단 배포 사용 시, 새로운 로직이 적용된 신규 애플리케이션의 실행 전후를 명확히 특정 오프셋 지점으로 나눌 수 있다는 장점이 있다
  - 신규 배포한 애플리케이션에 이슈가 발생해서 롤백할때 유용하다
  - 롤백을 통해 기존 애플리케이션으로 원복하고 데이터를 재처리하기 위해 **기존 애플리케이션이 처리 완료했던 오프셋으로 재지정하면 되기 때문**이다

#### 무중단 배포
- `블루/그린 배포` : 이전 버전 애플리케이션, 신규 버전 애플리케이션을 동시에 띄워놓고 트래픽 전환하는 방법
  - **파티션 개수와 컨슈머 개수를 동일하게 실행하는 애플리케이션을 운영할 때 유용**
    - 신규 버전 애플리케이션 배포 <br>→ 동일 컨슈머 그룹으로 파티션을 구독하도록 실행 <br>→ 신규 버전 애플리케이션의 컨슈머들은 파티션을 할당 받지 못하고 **유휴 상태(idle)로 기다릴 수 있음**
    > ❓ **동일 컨슈머 그룹으로 파티션을 구독하는 이유**
    > - **"트래픽 스위치 전까지 신규 버전이 메시지를 읽지 않고 대기"** 하도록 만들기 위해서 !
    > 
    > 예시 상황
    > - 블루/그린 배포 상황에서 기존(블루) 버전이 컨슈머 그룹 my-group으로 동작
    >   - 예: 파티션 3개, 컨슈머 3개 → 각자 1개씩 할당받음.
    > - 신규(그린) 버전을 띄우되 같은 컨슈머 그룹(my-group)으로 구독하게 하면 파티션 리밸런싱이 발생하지만, **기존 컨슈머가 이미 파티션을 모두 차지하고 있어서 신규 버전 컨슈머는 할당받을 파티션이 없음** → idle 상태
    >   - 이 상태에서는 트래픽 전환(예: 로드밸런서에서 블루 → 그린) 전까지 메시지를 처리하지 않고 안전하게 대기 가능
    >   - 그린 기동 시에 일어나는 일
    >     ```
    >     - 블루 컨슈머 3개가 P0, P1, P2를 하나씩 점유 중
    >     - 그린 컨슈머 3개가 같은 그룹에 들어옴 → 멤버 수가 3 → 6으로 증가
    >     - Kafka는 "멤버가 바뀌었네? 다시 분배해야지!" 하고 리밸런스를 트리거함
    >     - 하지만 파티션 수가 3개뿐이라, 균등 분배 로직상 일부 컨슈머는 파티션을 못 받는 상태가 나올 수 있음
    >       - 예: B1:P0, B2:P1, B3:P2, G1~G3:할당 없음
    >     - 이렇게 할당 결과에 변화가 없는(혹은 일부만 있는) 리밸런스가 끝남
    > 
    >     💡 "파티션을 받지 않는다"는 의미
    >     - 리밸런스가 발생했다고 해서 모든 컨슈머에 파티션이 반드시 재할당되는 건 아님
    >     - 파티션 수 < 컨슈머 수인 경우, 일부 컨슈머는 그냥 idle로 남게 됨
    >       - 이 idle 상태 컨슈머도 그룹의 멤버이긴 하지만, 실제 메시지는 소비하지 않음
    >     ```
    > - 반대로 다른 컨슈머 그룹으로 구독하게 하면
    >   - 신규 버전도 기존 버전과 동시에 같은 메시지를 읽게 됨
    >   - **메시지 중복 처리나 데이터 정합성 문제가 발생할 수 있음.**
  - 파티션 개수 != 컨슈머 개수일 경우, 일부 파티션은 기존 애플리케이션에 할당되고 일부 파티션은 신규 애플리케이션에 할당되어 섞이게 된다
    > 컨슈머 수 ≠ 파티션 수일 경우, 파티션 재분배가 발생해 블루/그린이 동시에 일부 파티션을 소비 → 메시지 처리가 섞이며 **파티션을 나눠 가져서 혼합 운영이 됨**
  - 리밸런싱이 발생하면서 파티션은 모두 신규 컨슈머와 연동된다.
    - 블루/그린 배포는 기존 컨슈머 중단 → 신규 컨슈머로 교체되는 순간에 **리밸런스가 한 번만 발생하기 때문에** 많은 수의 파티션을 운영하는 경우에도 짧은 리밸런스 시간으로 배포를 수행할수 있다.
- `롤링 배포` : 블루/그린 배포의 인스턴스 할당과 반환으로 인한 **리소스 낭비를 줄이면서 무중단 배포를 할 수 있다.**
  - 애플리케이션이 실행 중인 인프라를 완전히 교체하여 **이전 버전의 애플리케이션을 새로운 버전의 애플리케이션으로 서서히 교체**하는 배포 전략이므로 **리소스 낭비 줄일 수 있다**
  - 롤링 배포의 중요한 점은 파티션 개수가 인스턴스 개수와 같거나, 그보다 많아야 한다
    > - 파티션 수 < 인스턴스 수일 때, 롤링 배포 시 1개 인스턴스를 내리더라도 남은 인스턴스들이 파티션을 다 맡긴 하지만, idle 컨슈머 비율이 높아져 리소스 낭비
  - 롤링 배포의 경우, **인스턴스 1개를 교체하는 과정에서 내려갈 때 1번 올라올 때 1번씩, 총 리밸런스 2번 발생한다**
    > P0 ← C1 <br> 
    > P1 ← C2
    > 
    > 1. 첫번째 단계
    >  - C1을 종료 → 리밸런스 발생 (P0이 C2로 이동)
    >  - 신규 C1(그린)을 시작 → 다시 리밸런스 발생 (P0이 C1으로 재할당)
    > 2. 두번째 단계
    >  - C2(블루)를 종료 → 리밸런스 발생 (P1이 C1으로 이동)
    >  - 신규 C2(그린)를 시작 → 다시 리밸런스 발생 (P1이 C2로 재할당)
  - 파티션 개수가 많을수록 리밸런스 시간이 길어지므로, **파티션 개수가 많지 않은 경우에만 효과적인 방법이다**
- `카나리 배포` : 많은 데이터 중 일부분을 **신규 버전의 애플리케이션에 먼저 배포함으로써 이슈 없는지 사전 탐지가 가능하다**
  - 카나리 배포로 사전 테스트가 완료되면 나머지 99개 파티션에 할당된 컨슈머는 **롤링 또는 블루/그린 배포를 수행하여 무중단 배포가 가능하다**
  > ➕ 
  > 카나리 배포는 롤링 배포나 블루/그린 배포 같은 배포 방식과는 조금 다른 개념
  > - 카나리 배포 : 특정 일부(소수)의 트래픽이나 파티션을 먼저 신규 버전에 할당해서 실제 운영 환경에서 문제 없는지 **사전 검증하는 방법**
  > - 반면, 롤링 배포나 블루/그린 배포는 애플리케이션 전체를 점진적(롤링) 또는 환경 분리(블루/그린) 방식으로 새 버전으로 전환하는 **배포 전략**
  > - 즉, 카나리 배포는 롤링 배포나 블루/그린 배포를 수행할 때, **위험도를 낮추고 안정성을 높이기 위해 소수만 먼저 배포해 보는 “사전 검증(사전 테스트)” 단계 또는 기법으로 활용 가능**
  > 
  > ‘카나리 배포’는 “부분 배포 + 사전 검증”에 초점을 둔 방법이고, <br>‘롤링 배포’와 ‘블루/그린 배포’는 “점진적 버전 전환 방식”에 초점을 둔 개념이라 엄밀히 따지면 다르지만,<br> 배포 안정성을 위한 ‘점진적, 무중단’ 전략의 한 갈래로 많이 묶여서 설명됨

> ✅ **Kafka 파티션-컨슈머 매핑 규칙**
> - 하나의 파티션(P) → 같은 컨슈머 그룹 내에서 오직 한 컨슈머(C) 만 소비 가능
>   - 즉, 동일 그룹 안에서는 파티션의 소비자가 단 1명
>   - 파티션이 동시에 두 컨슈머에게 할당되는 일은 없음
> - 하나의 컨슈머(C) → 여러 개의 파티션(P) 를 동시에 소비 가능
>   - 컨슈머 수가 파티션 수보다 적으면 한 컨슈머가 여러 파티션을 맡게 됨
> 
> 즉, **"하나의 파티션 입장에서는, 같은 그룹 내에서 소비자는 단 하나"**
> <br>다만, 하나의 컨슈머가 여러 파티션을 맡는 건 가능하다

## 4.4. 스프링 카프카
### 4.4.1. 스프링 카프카 프로듀서
- 스프링 카프카 프로듀서는 `카프카 템플릿(Kafka Template)` 클래스를 사용하여 데이터를 전송할 수 있다.
  - 카프카 템플릿은 프로듀서 팩토리(ProducerFactory) 클래스를 통해 생성

#### 기본 카프카 템플릿
- 기본 카프카 템플릿은 기본 프로듀서 팩토리를 통해 생성된 카프카 템플릿을 사용한다. 
  - 기본 카프카 템플릿 사용 시 application.yaml에 프로듀서 옵션을 넣고 사용 가능

#### 커스텀 카프카 템플릿
- 커스텀 카프카 템플릿은 프로듀서 팩토리를 통해 만든 카프카 템플릿 객체를 빈으로 등록하여 사용한다.
```java
@Override
public void run(String... args) {
  ListenableFuture<SendResult<String, String>> future = customKafkaTemplate.send(TOPIC_NAME, "test");

  future.addCallback(new KafkaSendCallback<String, String>() {
    @Override
    public void onSuccess(SendResult<String, String> result) {}

    @Override
    public void onFailure(KafkaProducerException ex) {}
  });

  System.exit(0);
}
```
- 전송한 이후에 정상 적재됐는지 여부를 확인하고 싶다면 ListenableFuture 메서드를 사용하면 된다
  - `ListenableFuture` 인스턴스에 addCallback 함수를 붙여 **프로듀서가 보낸 데이터의 브로커 적재 여부를 비동기로 확인할 수 있다**.
  - 브로커에 정상 적재되었을 경우 onSuccess 메서드가 호출, 적재되지 않고 이슈가 발생했다면 onFailure 메서드 호출
